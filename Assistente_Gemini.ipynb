{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoqionuywZYusOFafMKKCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcelaFerreiraR/Assistente_Academico_Gemini/blob/main/Assistente_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exq505IvvbrI"
      },
      "outputs": [],
      "source": [
        "# C√©lula 1: Instala√ß√µes\n",
        "!pip install streamlit langchain langchain-google-genai langchain-community pypdf faiss-cpu pyngrok nest_asyncio -q\n",
        "print(\"Bibliotecas instaladas (incluindo langchain-community)!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 2: Imports e Configura√ß√£o da API Key\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "30Fw_8dHvhgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica patch para asyncio se necess√°rio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "JEvuv8niv4Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegar a API key do Colab Secrets\n",
        "try:\n",
        "    api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API Key n√£o encontrada ou vazia.\")\n",
        "    os.environ['GOOGLE_API_KEY'] = api_key\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\"‚úÖ Chave da API do Google configurada com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"üö® Erro ao configurar a API Key: {e}\")\n",
        "    print(\"üö® Verifique se voc√™ adicionou a 'GOOGLE_API_KEY' aos Secrets do Colab (√≠cone de chave üîë na barra lateral) e ativou 'Notebook access'.\")"
      ],
      "metadata": {
        "id": "mwsAeeQxv75u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baixar Bibliotecas\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "import sys\n",
        "\n",
        "print(\"--- Log 1: Script iniciado ---\", flush=True)\n",
        "\n",
        "# --- Configura√ß√£o da API Key ---\n",
        "api_key_in_app = os.getenv(\"GOOGLE_API_KEY\")\n",
        "genai_configured = False\n",
        "if api_key_in_app:\n",
        "    try:\n",
        "        genai.configure(api_key=api_key_in_app)\n",
        "        genai_configured = True\n",
        "        print(\"--- Log 2: genai.configure SUCESSO ---\", flush=True)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Erro configurando API: {e}\")\n",
        "        print(f\"--- Log 3: ERRO genai.configure: {e} ---\", flush=True)\n",
        "else:\n",
        "    st.error(\"API Key n√£o encontrada no ambiente.\")\n",
        "    print(\"--- Log 4: API Key N√ÉO encontrada ---\", flush=True)\n",
        "\n",
        "\n",
        "# --- Fun√ß√µes Auxiliares ---\n",
        "print(\"--- Log 5: Definindo fun√ß√µes auxiliares (com c√≥digo)... ---\", flush=True)\n",
        "\n",
        "def process_pdf(uploaded_file):\n",
        "    print(\"--- DEBUG process_pdf: Iniciando ---\", flush=True)\n",
        "    if uploaded_file is not None:\n",
        "        temp_file_path = f\"./{uploaded_file.name}\"\n",
        "        with open(temp_file_path, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "        print(f\"--- DEBUG process_pdf: PDF salvo temporariamente em {temp_file_path} ---\", flush=True)\n",
        "\n",
        "        loader = PyPDFLoader(temp_file_path)\n",
        "        try:\n",
        "            print(\"--- DEBUG process_pdf: Carregando e dividindo p√°ginas... ---\", flush=True)\n",
        "            pages = loader.load_and_split()\n",
        "            print(f\"--- DEBUG process_pdf: {len(pages)} p√°ginas carregadas ---\", flush=True)\n",
        "        except Exception as e:\n",
        "            print(f\"--- DEBUG process_pdf: ERRO no PyPDFLoader: {e} ---\", flush=True)\n",
        "            st.error(f\"Erro ao carregar o PDF com PyPDFLoader: {e}\")\n",
        "            if os.path.exists(temp_file_path): os.remove(temp_file_path)\n",
        "            return None\n",
        "\n",
        "        print(\"--- DEBUG process_pdf: Dividindo em chunks... ---\", flush=True)\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000, chunk_overlap=200, length_function=len, add_start_index=True\n",
        "        )\n",
        "        texts = text_splitter.split_documents(pages)\n",
        "        print(f\"--- DEBUG process_pdf: Texto dividido em {len(texts)} chunks ---\", flush=True)\n",
        "\n",
        "        if os.path.exists(temp_file_path): os.remove(temp_file_path)\n",
        "        print(\"--- DEBUG process_pdf: Arquivo tempor√°rio removido. Retornando chunks. ---\", flush=True)\n",
        "        return texts\n",
        "    print(\"--- DEBUG process_pdf: Nenhum arquivo enviado. Retornando None. ---\", flush=True)\n",
        "    return None\n",
        "\n",
        "def create_vector_store(text_chunks):\n",
        "    print(\"--- DEBUG create_vector_store: Iniciando ---\", flush=True)\n",
        "    if not text_chunks:\n",
        "        print(\"--- DEBUG create_vector_store: Nenhum chunk recebido. Retornando None. ---\", flush=True)\n",
        "        st.warning(\"Nenhum chunk de texto para vetorizar.\")\n",
        "        return None\n",
        "    try:\n",
        "        print(\"--- DEBUG create_vector_store: Criando embeddings... ---\", flush=True)\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        print(\"--- DEBUG create_vector_store: Construindo FAISS... ---\", flush=True)\n",
        "        vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)\n",
        "        print(\"--- DEBUG create_vector_store: FAISS constru√≠do com SUCESSO. Retornando store. ---\", flush=True)\n",
        "        return vector_store\n",
        "    except Exception as e:\n",
        "        print(f\"--- DEBUG create_vector_store: ERRO: {e} ---\", flush=True)\n",
        "        st.error(f\"Erro ao criar o banco de vetores: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_conversational_chain():\n",
        "    print(\"--- DEBUG get_conversational_chain: Iniciando ---\", flush=True)\n",
        "    prompt_template_str = \"\"\"\n",
        "    Voc√™ √© um assistente de IA especializado em responder perguntas com base em documentos acad√™micos (contexto) fornecidos.\n",
        "    Responda √† pergunta da forma mais detalhada e precisa poss√≠vel usando APENAS o contexto fornecido abaixo.\n",
        "    Cite trechos relevantes do contexto para embasar sua resposta sempre que poss√≠vel.\n",
        "    Se a resposta n√£o puder ser encontrada no contexto, diga explicitamente: \"Com base no contexto fornecido, n√£o consigo responder a essa pergunta.\"\n",
        "    N√£o invente informa√ß√µes.\n",
        "\n",
        "    Contexto:\n",
        "    {context}\n",
        "\n",
        "    Pergunta:\n",
        "    {question}\n",
        "\n",
        "    Resposta detalhada:\n",
        "    \"\"\"\n",
        "    # O importante √© que ele use {context} e {question}\n",
        "\n",
        "    print(\"--- DEBUG get_conversational_chain: Inicializando ChatGoogleGenerativeAI... ---\", flush=True)\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.2, convert_system_message_to_human=True)\n",
        "\n",
        "    print(\"--- DEBUG get_conversational_chain: Criando PromptTemplate... ---\", flush=True)\n",
        "    prompt = PromptTemplate(template=prompt_template_str, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "    print(\"--- DEBUG get_conversational_chain: Carregando load_qa_chain... ---\", flush=True)\n",
        "    chain = load_qa_chain(\n",
        "        llm=model,\n",
        "        chain_type=\"stuff\",\n",
        "        prompt=prompt,\n",
        "        document_variable_name=\"context\",\n",
        "        verbose=False\n",
        "    )\n",
        "    print(\"--- DEBUG get_conversational_chain: Cadeia criada com SUCESSO. Retornando chain. ---\", flush=True)\n",
        "    return chain\n",
        "\n",
        "print(\"--- Log 6: Fun√ß√µes auxiliares definidas (com c√≥digo) ---\", flush=True)\n",
        "\n",
        "# --- Interface Streamlit ---\n",
        "print(\"--- Log 7: Renderizando UI completa... ---\", flush=True)\n",
        "st.set_page_config(page_title=\"üéì Assistente Mestrado (Teste UI)\", layout=\"wide\")\n",
        "st.header(\"üéì Assistente de Mestrado com Gemini\")\n",
        "st.markdown(\"Fa√ßa upload de um PDF...\")\n",
        "\n",
        "# --- Estado da Sess√£o ---\n",
        "print(\"--- Log 8: Inicializando session_state... ---\", flush=True)\n",
        "if \"vector_store\" not in st.session_state: st.session_state.vector_store = None\n",
        "if \"pdf_processed_name\" not in st.session_state: st.session_state.pdf_processed_name = None\n",
        "if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "print(\"--- Log 9: session_state inicializado ---\", flush=True)\n",
        "\n",
        "\n",
        "# --- Barra Lateral ---\n",
        "print(\"--- Log 10: Renderizando sidebar... ---\", flush=True)\n",
        "with st.sidebar:\n",
        "    st.subheader(\"Seus Documentos\")\n",
        "    pdf_file = st.file_uploader(\"Carregue seu PDF aqui\", type=\"pdf\", accept_multiple_files=False)\n",
        "\n",
        "    # --- L√ìGICA DO BOT√ÉO PROCESSAR PDF - ATIVADA ---\n",
        "    if st.button(\"Processar PDF\") and pdf_file is not None:\n",
        "        print(f\"--- DEBUG Bot√£o Processar PDF CLICADO para: {pdf_file.name} ---\", flush=True)\n",
        "        if st.session_state.pdf_processed_name != pdf_file.name:\n",
        "             st.session_state.vector_store = None\n",
        "             st.session_state.messages = []\n",
        "             with st.spinner(f\"Processando '{pdf_file.name}'... Isso pode levar alguns minutos.\"):\n",
        "                text_chunks = process_pdf(pdf_file)\n",
        "                if text_chunks:\n",
        "                    st.session_state.vector_store = create_vector_store(text_chunks)\n",
        "                    if st.session_state.vector_store:\n",
        "                        st.session_state.pdf_processed_name = pdf_file.name\n",
        "                        st.success(f\"PDF '{pdf_file.name}' processado!\")\n",
        "                        print(f\"--- DEBUG PDF '{pdf_file.name}' processado com sucesso ---\", flush=True)\n",
        "                    else:\n",
        "                        st.error(\"Falha ao criar banco de vetores (FAISS). Verifique os logs.\")\n",
        "                        st.session_state.pdf_processed_name = None\n",
        "                        print(f\"--- DEBUG Falha em create_vector_store ---\", flush=True)\n",
        "                else:\n",
        "                    st.error(\"N√£o foi poss√≠vel extrair texto ou processar o PDF.\")\n",
        "                    st.session_state.pdf_processed_name = None\n",
        "                    print(f\"--- DEBUG Falha em process_pdf ---\", flush=True)\n",
        "        else:\n",
        "             st.info(f\"PDF '{pdf_file.name}' j√° foi processado anteriormente.\")\n",
        "\n",
        "\n",
        "    # --- ELIF  ---\n",
        "    elif pdf_file is None and st.session_state.vector_store is not None:\n",
        "\n",
        "         if st.button(\"Limpar PDF Carregado\"):\n",
        "              print(\"--- DEBUG Bot√£o Limpar PDF CLICADO ---\", flush=True)\n",
        "              st.session_state.vector_store = None\n",
        "              st.session_state.pdf_processed_name = None\n",
        "              st.session_state.messages = []\n",
        "              st.rerun()\n",
        "    # --- FIM DO ELIF ---\n",
        "\n",
        "    # --- Exibi√ß√£o do Status (Alinhado com if/elif) ---\n",
        "\n",
        "    if st.session_state.pdf_processed_name:\n",
        "        st.success(f\"PDF Ativo: {st.session_state.pdf_processed_name}\")\n",
        "    else:\n",
        "        st.info(\"Nenhum PDF processado.\")\n",
        "\n",
        "print(\"--- Log 11: Sidebar renderizada ---\", flush=True)\n",
        "\n",
        "\n",
        "# --- √Årea Principal de Chat ---\n",
        "print(\"--- Log 12: Renderizando √°rea principal... ---\", flush=True)\n",
        "st.subheader(\"Fa√ßa sua Pergunta\")\n",
        "user_question = st.text_input(\"Sua pergunta sobre o PDF:\", key=\"user_input\", disabled=not st.session_state.vector_store)\n",
        "\n",
        "# --- L√ìGICA DO BOT√ÉO ENVIAR PERGUNTA - ATIVADA ---\n",
        "if st.button(\"Enviar Pergunta\", disabled=not st.session_state.vector_store or not user_question):\n",
        "    print(f\"--- DEBUG Bot√£o Enviar Pergunta CLICADO com pergunta: {user_question} ---\", flush=True)\n",
        "    # --- DESCOMENTAR AS LINHAS ABAIXO ---\n",
        "    if st.session_state.vector_store and user_question:\n",
        "        with st.spinner(\"Pensando... üß†\"):\n",
        "            vector_store = st.session_state.vector_store\n",
        "            try:\n",
        "                # 1. Buscar documentos similares (Retrieval)\n",
        "                print(f\"--- DEBUG QA: Buscando documentos similares para: {user_question} ---\", flush=True)\n",
        "                docs = vector_store.similarity_search(user_question, k=5)\n",
        "                print(f\"--- DEBUG QA: Encontrados {len(docs)} documentos ---\", flush=True)\n",
        "\n",
        "                # 2. Chamar a cadeia de QA (Generation)\n",
        "                print(\"--- DEBUG QA: Obtendo cadeia de QA... ---\", flush=True)\n",
        "                chain = get_conversational_chain()\n",
        "                print(\"--- DEBUG QA: Executando cadeia de QA... ---\", flush=True)\n",
        "                response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
        "                answer = response[\"output_text\"]\n",
        "                print(\"--- DEBUG QA: Resposta recebida do LLM ---\", flush=True)\n",
        "\n",
        "                # 3. Mostrar a resposta\n",
        "                st.subheader(\"Resposta do Assistente:\")\n",
        "                st.markdown(answer)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_message = f\"Ocorreu um erro ao processar sua pergunta: {e}\"\n",
        "                print(f\"--- DEBUG QA: ERRO: {error_message} ---\", flush=True)\n",
        "                st.error(error_message)\n",
        "                if \"API key\" in str(e).lower() or \"permission\" in str(e).lower() or \"quota\" in str(e).lower():\n",
        "                     st.error(\"Erro de API ou Cota: Verifique sua chave, permiss√µes e limites de uso.\")\n",
        "                else:\n",
        "                     st.error(\"Um erro inesperado ocorreu.\")\n",
        "\n",
        "    elif not st.session_state.vector_store:\n",
        "        st.warning(\"Por favor, processe um PDF antes de fazer perguntas.\")\n",
        "    elif not user_question:\n",
        "         st.warning(\"Por favor, digite sua pergunta.\")\n",
        "# --- FIM DA L√ìGICA DO BOT√ÉO ENVIAR PERGUNTA ---\n",
        "\n",
        "print(\"--- Log 13: √Årea principal renderizada ---\", flush=True)\n",
        "\n",
        "# --- Rodap√© ---\n",
        "# ... (c√≥digo do rodap√©) ...\n",
        "print(\"--- Log 14: Fim do script ---\", flush=True)\n",
        "sys.stdout.flush()"
      ],
      "metadata": {
        "id": "rF4aKa4eydsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 4: Authtoken do Ngrok\n",
        "\n",
        "# Imports necess√°rios para esta c√©lula\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Mata processos anteriores se existirem (√∫til ao re-executar a c√©lula)\n",
        "!killall ngrok\n",
        "!killall streamlit\n",
        "\n",
        "# Define a porta padr√£o do Streamlit\n",
        "port = 8501\n",
        "\n",
        "# --- CONFIGURA√á√ÉO DO NGTOK AUTHTOKEN ---\n",
        "try:\n",
        "    authtoken = userdata.get('NGROK_AUTHTOKEN')\n",
        "    if not authtoken:\n",
        "        raise ValueError(\"NGROK_AUTHTOKEN n√£o encontrado ou vazio nos Secrets.\")\n",
        "    # Configura o authtoken no pyngrok\n",
        "    ngrok.set_auth_token(authtoken)\n",
        "    print(\"‚úÖ Authtoken do Ngrok configurado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"üö® Erro ao obter/configurar o Authtoken do Ngrok: {e}\")\n",
        "    print(\"üö® Verifique se voc√™ adicionou 'NGROK_AUTHTOKEN' aos Secrets do Colab (üîë) e ativou 'Notebook access'.\")\n",
        "    print(\"üö® Crie uma conta e obtenha seu token em https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "\n",
        "\n",
        "# --- Tenta iniciar o Ngrok e o Streamlit ---\n",
        "try:\n",
        "    # Inicia o ngrok em background para expor a porta escolhida\n",
        "    public_url = ngrok.connect(port)\n",
        "    print(\"=\"*50)\n",
        "    print(f\"‚úÖ Aplicativo Streamlit rodando! Acesse em: {public_url}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Executa o Streamlit em background usando nohup\n",
        "    # O '&' no final garante que ele rode em background\n",
        "    !nohup streamlit run app.py --server.port {port} --server.enableCORS=false --server.enableXsrfProtection=false &\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"üö® Erro ao iniciar o ngrok ou Streamlit: {e}\")\n",
        "    # Tenta desconectar se ngrok conectou mas streamlit falhou\n",
        "    active_tunnels = ngrok.get_tunnels()\n",
        "    for tunnel in active_tunnels:\n",
        "       ngrok.disconnect(tunnel.public_url)\n",
        "       print(f\"Tunnel {tunnel.public_url} desconectado.\")\n",
        "\n",
        "# A c√©lula continuar√° rodando para manter o servidor Streamlit e o t√∫nel ngrok ativos.\n",
        "# Para parar o aplicativo, voc√™ precisa interromper a execu√ß√£o desta c√©lula (bot√£o de parar)."
      ],
      "metadata": {
        "id": "0EtYdshnzR7p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}